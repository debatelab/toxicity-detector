config_version: 'v0.3'
used_chat_model: model_0
local_serialization: True
hf_base_path: 'datasets/DebateLabKIT/toxicity-detector-appdata'
result_data_path: 'logs/result_data'
local_base_path: 'C:/Users/admin/toxicity-detector'
log_path: 'logs'
subdirectory_construction: 'daily'
description: |
  Konfiguration mit Erläuterungen von "Stereotyp", "Ironie" und "ungerechtfertigt" und Frage nach Kontext beim Indikator "accusation"
system_prompt: | 
  You are a helpful assistant and an expert for the categorisation and annotation of texts.\n
  You read instructions carefully and follow them precisely. You give concise and clear answers.
# used data set for random example generation. currently one of:
# - problem_cases_20251203.csv (all cases up to the 3th of Dec that received feedback: 'incorrect' or 'absolutely_incorrect' or 'suspension')
# - toxicity_examples_detox_germeval21_n10000.csv
# - toxicity_examples_detox_germeval21_n5000.csv
toxicity_examples_data_file: 'toxicity_examples_detox_germeval21_n10000.csv'
toxicities:
  personalized_toxicity:
    title: "Personalisierte Toxizität"
    user_description: |
      **Personalisierte toxische Sprache** umfasst Äußerungen, die Beleidigung, Bedrohung oder 
      Belästigung einer Person beinhalten, gutheißen oder dazu aufrufen, *ohne Bezug auf die Zugehörigkeit* 
      der Person zu einer Bevölkerungsgruppe. 
    llm_description: |
      **Personalisierte toxische Sprache** umfasst Äußerungen, die Beleidigung, Bedrohung oder 
      Belästigung einer Person beinhalten, gutheißen oder dazu aufrufen, *ohne Bezug auf die Zugehörigkeit* 
      der Person zu einer Bevölkerungsgruppe.
    tasks:
      prepatory_analysis:
        general_questions: 
          name: "Allgemeine Fragen"
          llm_description: |
            - Werden im Text negative Begriffe verwendet? Wenn ja, welche?
            - Richtet sich der Text gegen eine einzelne Person oder eine Gruppe? Wenn, ja gegen wen?
            - Verwendet der Text Ironie, d.h. Sprache, bei der das eigentlich Gemeinte durch dessen Gegenteil ausgedrückt wird? Wenn ja, was ist die eigentliche Bedeutung des Texts?
            - Bezieht sich der Text auf eine andere Aussage, z.B. über ein Zitat, das durch Anführungszeichen gekennzeichnet wird? Wenn ja, wie wird die andere Aussage vom Text bewertet?
      indicator_analysis:
        sexual_harassment: 
          name: "Sexuelle Belästigung"
          llm_description: |
            Enthält die Äußerung sexuelle Kommentare über den Körper einer bestimmten Person, Aufforderungen zu sexuellen Handlungen, oder sonstige Inhalte, die eine bestimmte Person ohne deren Einwilligung anderweitig sexualisieren?
          # dummy (so far, the example fields are not used)
          positive_examples: ["... first example ... "]
          negative_examples: [" ... first negative example"]
        self_harm: 
          name: "Aufforderung zur Selbstgefährdung"
          llm_description: |
            Wird mit der Äußerung eine bestimmte Person zu selbstgefährdendem oder selbstverletzendem Verhalten aufgefordert?
        ridicule:
          name: "Verspottung"
          llm_description: |
            Wird sich über eine eine bestimmte Person oder eine Äußerung dieser lächerlich gemacht?
        threat:
          name: "Gewaltandrohung"
          llm_description: |
            Wird einer bestimmten Person mit Gewalt gedroht, zu Gewalt gegenüber dieser Person aufgerufen oder werden solche Gewalttaten gutgeheißen?
        insult:
          name: "Beleidigung"
          llm_description: |
            Wird eine bestimmte Person oder eine Gruppe einzelner Personen mit Schimpfwörtern oder Beleidigungen belegt?
        accusation:
          name: "Anschuldigung"
          llm_description: |
            Wird eine bestimmte Person ungerechtfertigt, d.h. ohne dass dies berechtigt wäre, einer Tat oder Absicht beschuldigt? Wenn ja, könnte die Beschuldigung je nach Kontext der Äußerung gerechtfertigt sein?
        exclusion:
          name: "Exkludierende Sprache"
          llm_description: |
            Soll mit der Äußerung eine bestimmte Person aus einem öffentlichen Diskurs ausgeschlossen werden oder dazu gebracht werden, sich nicht mehr an einem öffentlichen Diskurs zu beteiligen?
  hatespeech:
    title: "Gruppenbezogene Toxizität (Hassrede)"
    user_description: |
      Gruppenbezogene toxische Sprache ist Sprache, die aufgrund von Gruppenzugehörigkeit angreift oder herabsetzt, die zu Gewalt oder Hass gegen Gruppen (oder Personen) 
      aufruft und die auf bestimmten Merkmalen beruht wie körperlicher Erscheinung, Religion, Abstammung, nationale oder ethnische Herkunft, sexuelle Orientierung, 
      Geschlechtsidentität, politische Einstellung oder andere. Sie kann in verschiedenen Sprachstilen vorkommen, auch in subtilen Formen oder wenn Humor verwendet wird.
    llm_description: |
      Gruppenbezogene toxische Sprache ist Sprache, die aufgrund von Gruppenzugehörigkeit angreift oder herabsetzt, die zu Gewalt oder Hass gegen Gruppen (oder Personen) 
      aufruft und die auf bestimmten Merkmalen beruht wie körperlicher Erscheinung, Religion, Abstammung, nationale oder ethnische Herkunft, sexuelle Orientierung, 
      Geschlechtsidentität, politische Einstellung oder andere. Sie kann in verschiedenen Sprachstilen vorkommen, auch in subtilen Formen oder wenn Humor verwendet wird.
    tasks:
      prepatory_analysis:
        general_questions: 
          name: "Allgemeine Fragen"
          llm_description: |
            - Werden im Text negative Begriffe verwendet? Wenn ja, welche?
            - Richtet sich der Text gegen eine Gruppe oder gegen eine Person als Repräsentant einer Gruppe? Wenn, ja gegen wen?
            - Verwendet der Text Ironie, d.h. Sprache, bei der das eigentlich Gemeinte durch sein Gegenteil ausgedrückt wird? Wenn ja, was ist die eigentliche Bedeutung des Texts?
            - Bezieht sich der Text auf eine andere Aussage, z.B. über ein Zitat, das durch Anführungszeichen gekennzeichnet wird? Wenn ja, wie wird die andere Aussage vom Text bewertet?
      indicator_analysis:
        threat: 
          name: "Gewaltaufruf"
          llm_description: |
            Wird gegenüber einer Gruppe oder gegenüber einem Individuum als Repräsentant dieser Gruppe zu Gewalt 
            aufgerufen oder solche Gewalttaten gutgeheißen, auch durch Euphemismen oder implizit?
        victim_shaming: 
          name: "Opferverächtigung"
          llm_description: |
            Werden Opfer von Hasskriminalität oder das Konzept von Hassrede/Diskriminierung/Hasskriminalität gegenüber einer 
            Gruppe verächtlicht gemacht, verharmlost oder gerechtfertigt? 
        deadnaming:
          name: "Misgendering/Deadnaming"
          llm_description: |
            Enthält die Äußerung Misgendering oder Deadnaming einer Person?
        animal_comparison:
          name: "Herabwürdigender Tiervergleich"
          llm_description: |
            Wird eine Gruppe als Ganze oder ein Individuum als Repräsentant:in dieser Gruppe mit nicht-menschlichen 
            Tieren verglichen oder gleichgesetzt?
        desease_conflation:
          name: "Krankheitsgleichsetzung"
          llm_description: |
            Wird eine Gruppe als Ganze oder ein Individuum als Repräsentant:in dieser Gruppe mit einer Krankheit, Überträgern 
            von Krankheiten o.ä. gleichgesetzt? Wird mit (euphemistischen) Begriffen aus dem Wortfeld Krankheit zu Gewalt aufgerufen?
        stereotyping:
          name: "Stereotypisierung"
          llm_description: |
            Bestärkt die Äußerung schädigende Stereotype, d.h. verbreitete schädigende Vorurteile oder Klischees, gegenüber einer Gruppe?
        generalisation:
          name: "Generalisierung"
          llm_description: |
            Trifft die Äußerung generalisierte Aussagen über eine Gruppe oder leitet sie vom Verhalten oder den Eigenschaften eines einzelnen Individuums generalisierte Aussagen über eine Gruppe ab?
        arbitrariness:
          name: "Aufruf zur gesetzlichen Willkür"
          llm_description: | 
            Wird zu gesetzlicher Willkür gegenüber einer Gruppe aufgerufen?
        accusation:
          name: "Anschuldigung"
          llm_description: |
            Wird eine Gruppe als Ganze oder ein Individuum als Repräsentant:in dieser Gruppe ungerechtfertigt, d.h. ohne dass dies berechtigt wäre, einer Tat oder Absicht beschuldigt? Wenn ja, könnte die Beschuldigung je nach Kontext der Äußerung gerechtfertigt sein?
        exclusion:
          name: "Exkludierende Sprache"
          llm_description: |
            Soll mit der Äußerung eine Gruppe aus einem öffentlichen Diskurs ausgeschlossen werden oder ihr gesellschaftliche Teilhabe verweigert werden?
        insult:
          name: "Beleidigung"
          llm_description: |
            Werden negativ besetzte Ausdrücke als Platzhalter für eine Gruppe verwendet oder eine Gruppe als Ganze oder ein Individuum als Repräsentant:in mit Schimpfwörtern belegt?
  # TODO
  # other_toxic_speech:
  #   title: "Anderweitige Toxizität"
  #   user_description: |
  #     **Anderweitige toxische Sprache** umfasst Äußerungen, die sich nicht negativ gegen Gruppen oder Personen richten, aber trotzdem die Diskussionatmosphäre negativ beeinflussen. Dazu gehört beispielsweise der übermäßige Gebrauch von Schimpfwörtern.
  #   llm_description: |
  #     **Anderweitige toxische Sprache** umfasst Äußerungen, die sich nicht negativ gegen Gruppen oder Personen richten, aber trotzdem die Diskussionatmosphäre negativ beeinflussen. Dazu gehört beispielsweise der übermäßige Gebrauch von Schimpfwörtern.
models:
  model_0:
    name: "Llama-3.2-3B over together.ai"
    description: "NVIDEA NIM API (kostenpflichtig über DebateLab Account)"
    base_url: "https://router.huggingface.co/together/v1"
    model: "meta-llama/Llama-3.2-3B-Instruct-Turbo"
    api_key_name: "hf_debatelab_inference_provider"
    llm_chain: "chat-chain"
    model_kwargs:
      max_tokens: 512
      temperature: 0.2
      extra_headers: 
        X-HF-Bill-To: "DebateLabKIT"
  model_1:
    # Currently, does not work. Throws a BadRequestError: 
    # openai.BadRequestError: Error code: 400 - {'type': 'about:blank', 'status': 400, 'title': 'Bad Request', 'detail': 'Inference error'}
    name: "Mixtral-8x7B-Instruct-v0.1"
    description: "NVIDEA NIM API (kostenpflichtig über DebateLab Account)"
    base_url: "https://huggingface.co/api/integrations/dgx/v1"
    model: "mistralai/Mixtral-8x7B-Instruct-v0.1"
    api_key_name: "kideku_toxicity_app_nim"
    llm_chain: "chat-chain"
    max_tokens: 512
    temperature: 0.2
  model_2:
    name: "Llama-3.1-70B-Instruct"
    description: "NVIDEA NIM API (kostenpflichtig über DebateLab Account)"
    base_url: "https://huggingface.co/api/integrations/dgx/v1"
    model: "meta-llama/Llama-3.1-70B-Instruct"
    api_key_name: "kideku_toxicity_app_nim"
    llm_chain: "chat-chain"
    max_tokens: 1024
    temperature: 0.2
  model_3:
    name: "Meta-Llama-3-8B-Instruct"
    description: "NVIDEA NIM API (kostenpflichtig über DebateLab Account)"
    base_url: "https://huggingface.co/api/integrations/dgx/v1"
    model: "meta-llama/Meta-Llama-3-8B-Instruct"
    api_key_name: "kideku_toxicity_app_nim"
    llm_chain: "chat-chain"
    max_tokens: 512
    temperature: 0.2
  model_4_old:
    name: "Mistral-7B-Instruct-v0.2"
    description: "Zum Testen (freie Inference API, derzeit über den privaten Token von SC)"
    base_url: "https://api-inference.huggingface.co/v1/"
    #repo_id: "mistralai/Mistral-7B-Instruct-v0.2"
    model: "mistralai/Mistral-7B-Instruct-v0.2"
    llm_chain: "chat-chain"
    api_key_name: "HF_TOKEN_KIDEKU_INFERENCE"
    max_tokens: 1024
    temperature: 0.2
  # with the new InferenceProviders baseUrls changed on HF
  model_4:
    name: "Mistral-7B-Instruct-v0.3"
    description: "Zum Testen (freie Inference API, derzeit über den privaten Token von SC)"
    base_url: "https://router.huggingface.co/hf-inference/v1/"
    #repo_id: "mistralai/Mistral-7B-Instruct-v0.2"
    model: "mistralai/Mistral-7B-Instruct-v0.3"
    llm_chain: "chat-chain"
    api_key_name: "HF_TOKEN_KIDEKU_INFERENCE"
    max_tokens: 1024
    temperature: 0.2
  model_5:
    name: "tgi@kriton"
    description: "TGI Server vom ITZ zum Testen. (im Moment bitte nicht nehmen)"
    base_url: "http://kriton.philosophie.kit.edu:8080/v1/"
    model: "tgi"
    api_key: "no-key-required"
    llm_chain: "chat-chain"
    max_tokens: 1024
    temperature: 0.2
  model_6:
    name: "deberta-v3-base-zeroshot-v2.0 (zero-shot categorization)"
    description: "Zeroshot Classifier (kann man im Moment nicht nehmen)"
    repo_id: "MoritzLaurer/deberta-v3-base-zeroshot-v2.0"
    llm_chain: "zero-shot-chain"
  model_7:
    name: "Llama-3.1-70B-Instruct"
    description: "HF dedicated endpoint (debatelab)"
    base_url: "https://ev6086dt6s7nn1b5.us-east-1.aws.endpoints.huggingface.cloud/v1/"
    model: "meta-llama/Llama-3.1-70B-Instruct"
    api_key_name: "HF_ENDPOINT_TOKEN_TOXICITY_DETECTOR"
    backend_type: "tgi"
    max_tokens: 2048
    temperature: 0.2
  model_8:
    # local model
    name: "deepseek-r1-distill-qwen-7b"
    description: "HF dedicated endpoint (debatelab)"
    base_url: "http://localhost:1234/v1/"
    model: "deepseek-r1-distill-qwen-7b"
    llm_chain: "chat-chain"
    api_key: "no-key-required"
    max_tokens: 1024
    temperature: 0.2
  model_9:
    # local model
    name: "llama-3.1-argunaut-1-8b-spin"
    description: "local OPenAI-like endpoint using LMStudio"
    base_url: "http://localhost:1234/v1/"
    model: "meta-llama-3.1-8b-instruct"
    llm_chain: "chat-chain"
    api_key: "no-key-required"
    max_tokens: 1024
    temperature: 0.2
  
# =============================================================================
# MODEL SELECTION
# =============================================================================
# Specifies which model from the 'models' section below should be used for 
# toxicity detection. Must match one of the keys in the 'models' dictionary.
used_chat_model: Llama-3.2-3B (local)

# =============================================================================
# DATA STORAGE CONFIGURATION
# =============================================================================
# Controls where detection results and logs are saved.

# local_serialization: If True, saves results to local filesystem. 
# If False, saves to HuggingFace Hub (requires hf_base_path and hf_key_name).
local_serialization: True

# Uncomment if needed: HuggingFace dataset path for storing results when 
# local_serialization is False (e.g., 'datasets/username/dataset-name').
# hf_base_path: 'datasets/username/dataset-name'

# local_base_path: Root directory for local storage when local_serialization 
# is True. All paths (result_data_path, log_path) are relative to this.
local_base_path: '.'

# result_data_path: Directory for saving detection results (relative to base_path).
# Each result is saved as a YAML file named with a UUID.
result_data_path: 'result_data'

# log_path: Directory for application logs (relative to base_path).
# Logs are automatically rotated daily and retained for 30 days.
log_path: 'logs'

# =============================================================================
# ENVIRONMENT & API KEYS
# =============================================================================
# env_file: Path to .env file containing API keys for model providers.
# Keys are referenced by name in the 'models' section (see api_key_name field).
env_file: '.env'

# =============================================================================
# RESULT ORGANIZATION
# =============================================================================
# subdirectory_construction: Automatically organizes results into subdirectories.
# Options: 'daily', 'weekly', 'monthly', 'yearly', or null for no subdirectories.
# Example: 'daily' creates folders like 'result_data/2026_01_08/'
subdirectory_construction: 'daily'

# =============================================================================
# CONFIGURATION METADATA
# =============================================================================
# config_version: Version of the configuration format. Must be >= v0.3.
# Used to ensure compatibility with the toxicity detector code.
# Remark: Configs of version v0.3 should be compatible with v0.4 
config_version: 'v0.4'

# description: Human-readable description of this specific configuration.
# Useful for documenting what makes this config unique or its intended use case.
description: |
  Configuration of the Toxicity Detector pipeline with ...

# =============================================================================
# LLM SYSTEM PROMPT
# =============================================================================
# system_prompt: Instructions sent to the LLM before each analysis request.
# Sets the tone and expertise level for the model's responses.
system_prompt: | 
  You are a helpful assistant and an expert for the categorisation and annotation of texts.
  You read instructions carefully and follow them precisely.
  You give concise and clear answers.

# =============================================================================
# TOXICITY TYPE DEFINITIONS
# =============================================================================
# toxicities: Defines different types of toxicity that can be detected.
# Each toxicity type has:
# - title: Display name for users
# - user_description: Explanation shown to end users
# - llm_description: Definition provided to the LLM for analysis
# - tasks: Analysis tasks including preparatory questions and specific indicators
# 
# ATTENTION: For now, you cannot define additional toxicity types here. However,
# you cann add, remove or change the specific indicators for both toxicity types
# the general questions that are used in the prepatory analysis.
# 
toxicities:
  # ---------------------------------------------------------------------------
  # PERSONALIZED TOXICITY
  # ---------------------------------------------------------------------------
  # Toxicity directed at specific individuals without reference to group identity.
  # Includes insults, threats, harassment, sexual harassment, etc.
  personalized_toxicity:
    title: "Personalisierte Toxizität"
    user_description: |
      **Personalisierte toxische Sprache** umfasst Äußerungen, die Beleidigung, Bedrohung oder 
      Belästigung einer Person beinhalten, gutheißen oder dazu aufrufen, *ohne Bezug auf die Zugehörigkeit* 
      der Person zu einer Bevölkerungsgruppe. 
    llm_description: |
      **Personalisierte toxische Sprache** umfasst Äußerungen, die Beleidigung, Bedrohung oder 
      Belästigung einer Person beinhalten, gutheißen oder dazu aufrufen, *ohne Bezug auf die Zugehörigkeit* 
      der Person zu einer Bevölkerungsgruppe.
    tasks:
      prepatory_analysis:
        general_questions: 
          name: "Allgemeine Fragen"
          llm_description: |
            - Werden im Text negative Begriffe verwendet? Wenn ja, welche?
            - Richtet sich der Text gegen eine einzelne Person oder eine Gruppe? Wenn, ja gegen wen?
            - Verwendet der Text Ironie, d.h. Sprache, bei der das eigentlich Gemeinte durch dessen Gegenteil ausgedrückt wird? Wenn ja, was ist die eigentliche Bedeutung des Texts?
            - Bezieht sich der Text auf eine andere Aussage, z.B. über ein Zitat, das durch Anführungszeichen gekennzeichnet wird? Wenn ja, wie wird die andere Aussage vom Text bewertet?
      indicator_analysis:
        sexual_harassment: 
          name: "Sexuelle Belästigung"
          llm_description: |
            Enthält die Äußerung sexuelle Kommentare über den Körper einer bestimmten Person, Aufforderungen zu sexuellen Handlungen, oder sonstige Inhalte, die eine bestimmte Person ohne deren Einwilligung anderweitig sexualisieren?
          # dummy (so far, the example fields are not used)
          positive_examples: ["... first example ... "]
          negative_examples: [" ... first negative example"]
        self_harm: 
          name: "Aufforderung zur Selbstgefährdung"
          llm_description: |
            Wird mit der Äußerung eine bestimmte Person zu selbstgefährdendem oder selbstverletzendem Verhalten aufgefordert?
        ridicule:
          name: "Verspottung"
          llm_description: |
            Wird sich über eine eine bestimmte Person oder eine Äußerung dieser lächerlich gemacht?
        threat:
          name: "Gewaltandrohung"
          llm_description: |
            Wird einer bestimmten Person mit Gewalt gedroht, zu Gewalt gegenüber dieser Person aufgerufen oder werden solche Gewalttaten gutgeheißen?
        insult:
          name: "Beleidigung"
          llm_description: |
            Wird eine bestimmte Person oder eine Gruppe einzelner Personen mit Schimpfwörtern oder Beleidigungen belegt?
        accusation:
          name: "Anschuldigung"
          llm_description: |
            Wird eine bestimmte Person ungerechtfertigt, d.h. ohne dass dies berechtigt wäre, einer Tat oder Absicht beschuldigt? Wenn ja, könnte die Beschuldigung je nach Kontext der Äußerung gerechtfertigt sein?
        exclusion:
          name: "Exkludierende Sprache"
          llm_description: |
            Soll mit der Äußerung eine bestimmte Person aus einem öffentlichen Diskurs ausgeschlossen werden oder dazu gebracht werden, sich nicht mehr an einem öffentlichen Diskurs zu beteiligen?
  
  # ---------------------------------------------------------------------------
  # HATESPEECH (GROUP-BASED TOXICITY)
  # ---------------------------------------------------------------------------
  # Toxicity targeting groups or individuals based on group membership.
  # Includes hate speech, discrimination based on identity characteristics
  # (appearance, religion, ethnicity, sexual orientation, gender identity, etc.).
  hatespeech:
    title: "Gruppenbezogene Toxizität (Hassrede)"
    user_description: |
      Gruppenbezogene toxische Sprache ist Sprache, die aufgrund von Gruppenzugehörigkeit angreift oder herabsetzt, die zu Gewalt oder Hass gegen Gruppen (oder Personen) 
      aufruft und die auf bestimmten Merkmalen beruht wie körperlicher Erscheinung, Religion, Abstammung, nationale oder ethnische Herkunft, sexuelle Orientierung, 
      Geschlechtsidentität, politische Einstellung oder andere. Sie kann in verschiedenen Sprachstilen vorkommen, auch in subtilen Formen oder wenn Humor verwendet wird.
    llm_description: |
      Gruppenbezogene toxische Sprache ist Sprache, die aufgrund von Gruppenzugehörigkeit angreift oder herabsetzt, die zu Gewalt oder Hass gegen Gruppen (oder Personen) 
      aufruft und die auf bestimmten Merkmalen beruht wie körperlicher Erscheinung, Religion, Abstammung, nationale oder ethnische Herkunft, sexuelle Orientierung, 
      Geschlechtsidentität, politische Einstellung oder andere. Sie kann in verschiedenen Sprachstilen vorkommen, auch in subtilen Formen oder wenn Humor verwendet wird.
    tasks:
      prepatory_analysis:
        general_questions: 
          name: "Allgemeine Fragen"
          llm_description: |
            - Werden im Text negative Begriffe verwendet? Wenn ja, welche?
            - Richtet sich der Text gegen eine Gruppe oder gegen eine Person als Repräsentant einer Gruppe? Wenn, ja gegen wen?
            - Verwendet der Text Ironie, d.h. Sprache, bei der das eigentlich Gemeinte durch sein Gegenteil ausgedrückt wird? Wenn ja, was ist die eigentliche Bedeutung des Texts?
            - Bezieht sich der Text auf eine andere Aussage, z.B. über ein Zitat, das durch Anführungszeichen gekennzeichnet wird? Wenn ja, wie wird die andere Aussage vom Text bewertet?
      indicator_analysis:
        threat: 
          name: "Gewaltaufruf"
          llm_description: |
            Wird gegenüber einer Gruppe oder gegenüber einem Individuum als Repräsentant dieser Gruppe zu Gewalt 
            aufgerufen oder solche Gewalttaten gutgeheißen, auch durch Euphemismen oder implizit?
        victim_shaming: 
          name: "Opferverächtigung"
          llm_description: |
            Werden Opfer von Hasskriminalität oder das Konzept von Hassrede/Diskriminierung/Hasskriminalität gegenüber einer 
            Gruppe verächtlicht gemacht, verharmlost oder gerechtfertigt? 
        deadnaming:
          name: "Misgendering/Deadnaming"
          llm_description: |
            Enthält die Äußerung Misgendering oder Deadnaming einer Person?
        animal_comparison:
          name: "Herabwürdigender Tiervergleich"
          llm_description: |
            Wird eine Gruppe als Ganze oder ein Individuum als Repräsentant:in dieser Gruppe mit nicht-menschlichen 
            Tieren verglichen oder gleichgesetzt?
        desease_conflation:
          name: "Krankheitsgleichsetzung"
          llm_description: |
            Wird eine Gruppe als Ganze oder ein Individuum als Repräsentant:in dieser Gruppe mit einer Krankheit, Überträgern 
            von Krankheiten o.ä. gleichgesetzt? Wird mit (euphemistischen) Begriffen aus dem Wortfeld Krankheit zu Gewalt aufgerufen?
        stereotyping:
          name: "Stereotypisierung"
          llm_description: |
            Bestärkt die Äußerung schädigende Stereotype, d.h. verbreitete schädigende Vorurteile oder Klischees, gegenüber einer Gruppe?
        generalisation:
          name: "Generalisierung"
          llm_description: |
            Trifft die Äußerung generalisierte Aussagen über eine Gruppe oder leitet sie vom Verhalten oder den Eigenschaften eines einzelnen Individuums generalisierte Aussagen über eine Gruppe ab?
        arbitrariness:
          name: "Aufruf zur gesetzlichen Willkür"
          llm_description: | 
            Wird zu gesetzlicher Willkür gegenüber einer Gruppe aufgerufen?
        accusation:
          name: "Anschuldigung"
          llm_description: |
            Wird eine Gruppe als Ganze oder ein Individuum als Repräsentant:in dieser Gruppe ungerechtfertigt, d.h. ohne dass dies berechtigt wäre, einer Tat oder Absicht beschuldigt? Wenn ja, könnte die Beschuldigung je nach Kontext der Äußerung gerechtfertigt sein?
        exclusion:
          name: "Exkludierende Sprache"
          llm_description: |
            Soll mit der Äußerung eine Gruppe aus einem öffentlichen Diskurs ausgeschlossen werden oder ihr gesellschaftliche Teilhabe verweigert werden?
        insult:
          name: "Beleidigung"
          llm_description: |
            Werden negativ besetzte Ausdrücke als Platzhalter für eine Gruppe verwendet oder eine Gruppe als Ganze oder ein Individuum als Repräsentant:in mit Schimpfwörtern belegt?

# =============================================================================
# MODEL CONFIGURATIONS
# =============================================================================
# models: Dictionary of available LLM configurations.
# Each model requires:
# - name: Display name (used in the Gradio app)
# - description: Purpose or provider info (optional)
# - base_url: API endpoint URL
# - model: Model identifier (repo ID or endpoint-specific name)
# - llm_chain: Type of chain (for now, only "chat-chain" is available)
# - api_key or api_key_name: Authentication (direct key or env var name)
# - model_kwargs: Additional parameters (max_tokens, temperature, etc.)
#
# ATTENTION: For security reason, we discourage to put API keys in this 
# file. Instead, use the option to specify the name for an environment variable 
# via `api_key_name` that stores the API key.
#
# The model specified in 'used_chat_model' at the top determines which 
# configuration is actually used during toxicity detection.
# You can specify different models and switch easily between them via
# the `used_chat_model` or via a dropdown menu in the Gradio app.
models:
  # ---------------------------------------------------------------------------
  # Example model configuration: Served locally via LM Studio (https://lmstudio.ai/) 
  # ---------------------------------------------------------------------------
  Llama-3.2-3B (local):
    name: "Llama-3.2-3B (local)"
    description: "Llama-3.2-3B over LM Studio"
    base_url: "http://127.0.0.1:1234/v1"  # Local server
    model: "llama-3.2-3b-instruct"
    llm_chain: "chat-chain"
    api_key: no-key-needed  # Direct API key (use for local servers without auth)
    model_kwargs:
      max_tokens: 512
      temperature: 0.2

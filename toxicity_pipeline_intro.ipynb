{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toxicity Detection Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The toxicity detection pipeline is designed to identify and classify toxic content in text data. \n",
    "\n",
    "+ The pipeline can be configured using YAML files, or programmatically as shown below.\n",
    "+ There are different ways to run the pipeline: You can use the `detect_toxicity` function directly, or set up a Gradio web application as shown in the README.md.\n",
    "\n",
    "This notebook demonstrates **how to set up and run the pipeline using a Hugging Face API key**. There are several possibilities to execute this notebook. You can, for instance,\n",
    "\n",
    "1. execute this notebook on Colab: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/debatelab/toxicity-detector/blob/master/toxicity_pipeline_intro.ipynb), or\n",
    "2. execute this notebook locally in, for instance, [JupyterLab](https://jupyter.org/). The second option requires you to have Python installed (specifics ) installed on your machine.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites: Using Hugging Face API Key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install toxicity-detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration of the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toxicity_detector import detect_toxicity\n",
    "from toxicity_detector.config import PipelineConfig\n",
    "\n",
    "pipeline_config = PipelineConfig(\n",
    "    # We use the Llama-3.2-3B model via the Hugging Face API\n",
    "    # This model is very small and only suitable for demonstration purposes.\n",
    "    # For serious use cases, consider using a larger and newer model.\n",
    "    used_chat_model='Llama-3.2-3B',\n",
    "    local_base_path='.',\n",
    "    result_data_path='result_data',\n",
    "    log_path='logs',\n",
    "    models={\n",
    "        'Llama-3.2-3B': {\n",
    "            'name': 'Llama-3.2-3B',\n",
    "            'description': 'Llama-3.2-3B over together.ai',\n",
    "            'base_url': 'https://router.huggingface.co/together/v1',\n",
    "            'model': 'meta-llama/Llama-3.2-3B-Instruct-Turbo',\n",
    "            # Passing the API key directly here for demonstration purposes. \n",
    "            # In practice, you should use environment variables an pass here the \n",
    "            # name of the variable that holds the key, e.g.,\n",
    "            # 'api_key_name': '<YOUR_ENV_VARIABLE_NAME>',\n",
    "            'api_key': '<YOUR_HUGGINGFACE_API_KEY>',\n",
    "            'llm_chain': 'chat-chain',\n",
    "            'model_kwargs': {\n",
    "                'max_tokens': 1024,\n",
    "                'temperature': 0.2,\n",
    "            }\n",
    "        },\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the pipeline via `detect_toxicity`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `detect_toxicity` will run the toxicity detection pipeline on the provided input text, return the reulst and store them in the specified result data path (the YAML file that contains the result contains all information to reproduce the run and the full reasoning trace)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "input_text = 'Peter is dumn.'\n",
    "\n",
    "result = detect_toxicity(\n",
    "    input_text=input_text,\n",
    "    user_input_source=None, \n",
    "    toxicity_type='personalized_toxicity',\n",
    "    context_info=None,\n",
    "    pipeline_config=pipeline_config,\n",
    "    serialize_result=True,\n",
    ")\n",
    "\n",
    "# Display results as formatted markdown\n",
    "analysis = result.answer['analysis_result']\n",
    "contains_toxicity = result.answer['contains_toxicity']\n",
    "\n",
    "markdown_output = f\"\"\"\n",
    "## üîç Toxicity Analysis Results\n",
    "\n",
    "### Input Text\n",
    "> {input_text}\n",
    "\n",
    "### Contains Toxicity\n",
    "**{'‚ö†Ô∏è YES' if contains_toxicity else '‚úÖ NO'}**\n",
    "\n",
    "### Analysis Result\n",
    "{analysis}\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(markdown_output))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "toxicity-detector",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

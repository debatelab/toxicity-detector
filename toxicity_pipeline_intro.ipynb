{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toxicity Detection Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The toxicity detection pipeline is designed to identify and classify toxic content in text data. \n",
    "\n",
    "+ The pipeline can be configured using YAML files, or programmatically as shown below.\n",
    "+ There are different ways to run the pipeline: You can use the `detect_toxicity` function directly, or set up a Gradio web application as shown in the [README.md](https://github.com/debatelab/toxicity-detector/blob/main/README.md).\n",
    "\n",
    "This notebook demonstrates **how to set up and run the pipeline using a Hugging Face API key**. There are several possibilities to execute this notebook. You can, for instance,\n",
    "\n",
    "1. execute this notebook on Colab: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/debatelab/toxicity-detector/blob/master/toxicity_pipeline_intro.ipynb), or\n",
    "2. execute this notebook locally in, for instance, [JupyterLab](https://jupyter.org/). The second option requires you to have Python installed installed on your machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites: Using Hugging Face API Key\n",
    "\n",
    "In this notebook, we will use models served through Hugging Face that require authentication via an API key. \n",
    "\n",
    "1. If you don't have a Hugging Face account yet, you can create one for free at [huggingface.co/join](https://huggingface.co/join). \n",
    "2. After creating an account, you can generate an API key by navigating to your account settings at [huggingface.co/settings/tokens](https://huggingface.co/settings/tokens) and clicking on \"New token\". Make sure to copy the generated token, as you will need it to authenticate your requests.\n",
    "\n",
    "Alternatively, you can configure the pipeline to use another model. Please refer to the [example configuration YAML file](https://github.com/debatelab/toxicity-detector/blob/main/config/pipeline_config.yaml) for details on how to set up different models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install toxicity-detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration of the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toxicity_detector import detect_toxicity\n",
    "from toxicity_detector.config import PipelineConfig\n",
    "\n",
    "pipeline_config = PipelineConfig(\n",
    "    # We use the Llama-3.2-3B model via the Hugging Face API\n",
    "    # This model is very small and only suitable for demonstration purposes.\n",
    "    # For serious use cases, consider using a larger and newer model.\n",
    "    used_chat_model='Llama-3.2-3B',\n",
    "    local_base_path='.',\n",
    "    result_data_path='result_data',\n",
    "    log_path='logs',\n",
    "    models={\n",
    "        'Llama-3.2-3B': {\n",
    "            'name': 'Llama-3.2-3B',\n",
    "            'description': 'Llama-3.2-3B over together.ai',\n",
    "            'base_url': 'https://router.huggingface.co/together/v1',\n",
    "            'model': 'meta-llama/Llama-3.2-3B-Instruct-Turbo',\n",
    "            # Passing the API key directly here for demonstration purposes. \n",
    "            # In practice, you should use environment variables an pass here the \n",
    "            # name of the variable that holds the key, e.g.,\n",
    "            # 'api_key_name': '<YOUR_ENV_VARIABLE_NAME>',\n",
    "            'api_key': '<YOUR_HUGGINGFACE_API_KEY>',\n",
    "            'llm_chain': 'chat-chain',\n",
    "            'model_kwargs': {\n",
    "                'max_tokens': 1024,\n",
    "                'temperature': 0.2,\n",
    "            }\n",
    "        },\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the pipeline via `detect_toxicity`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `detect_toxicity` will run the toxicity detection pipeline on the provided input text, return the result and store it in the specified result data path (the YAML file that contains the result contains all information to reproduce the run and the full reasoning trace)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "input_text = 'Peter is dumn.'\n",
    "\n",
    "result = detect_toxicity(\n",
    "    input_text=input_text,  # The text to be analyzed\n",
    "    user_input_source=None,  # Optional: identifier for the source of the input (e.g., 'chat', 'comment')\n",
    "    toxicity_type='hatespeech',  # Type of toxicity analysis to perform (one of: 'hatespeech' or 'personalized_toxicity')\n",
    "    context_info=None,  # Optional: additional context about the conversation or situation\n",
    "    pipeline_config=pipeline_config,  # Configuration specifying model, paths, and behavior\n",
    "    serialize_result=True,  # If True, saves the result to disk as YAML\n",
    ")\n",
    "\n",
    "# Display results as formatted markdown\n",
    "analysis = result.answer['analysis_result']\n",
    "contains_toxicity = result.answer['contains_toxicity']\n",
    "\n",
    "# Map toxicity values to display strings\n",
    "toxicity_display = {\n",
    "    ToxicityAnswer.FALSE: 'âœ… NO',\n",
    "    ToxicityAnswer.TRUE: 'âš ï¸ YES',\n",
    "    ToxicityAnswer.UNCLEAR: 'â“ UNKNOWN/UNCLEAR'\n",
    "}\n",
    "\n",
    "contains_toxicity_str = toxicity_display.get(contains_toxicity, 'Answer could not be parsed')\n",
    "\n",
    "markdown_output = f\"\"\"\n",
    "## ðŸ” Toxicity Analysis Results\n",
    "\n",
    "### Input Text\n",
    "> {input_text}\n",
    "\n",
    "### Contains Toxicity\n",
    "**{contains_toxicity_str}**\n",
    "\n",
    "### Analysis Result\n",
    "{analysis}\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(markdown_output))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "toxicity-detector",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toxicity Detection Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The toxicity detection pipeline is designed to identify and classify toxic content in text data. \n",
    "\n",
    "+ The pipeline can be configured using YAML files, or programmatically as shown below.\n",
    "+ There are different ways to run the pipeline: You can use the `detect_toxicity` function directly, or set up a Gradio web application as shown in the README.md.\n",
    "\n",
    "This notebook demonstrates **how to set up and run the pipeline using a Hugging Face API key**. There are several possibilities to execute this notebook. You can, for instance,\n",
    "\n",
    "1. execute this notebook on Colab: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/debatelab/toxicity-detector/blob/master/toxicity_pipeline_intro.ipynb), or\n",
    "2. execute this notebook locally in, for instance, [JupyterLab](https://jupyter.org/). The second option requires you to have Python installed (specifics ) installed on your machine.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites: Using Hugging Face API Key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install toxicity-detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration of the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sebastian/Nextcloud/Documents/mindmaps/mind/projects/kideku/code/toxicity-detector/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\u001b[32m2026-01-07 21:24:08.806\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtoxicity_detector.config\u001b[0m:\u001b[36mload_env_file\u001b[0m:\u001b[36m183\u001b[0m - \u001b[1mLoaded environment variables from '.env'\u001b[0m\n",
      "\u001b[32m2026-01-07 21:24:08.810\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtoxicity_detector.config\u001b[0m:\u001b[36m_configure_logging\u001b[0m:\u001b[36m156\u001b[0m - \u001b[1mConfigured logging to file: ./logs/toxicity_detector_log_2026_01_07.log\u001b[0m\n",
      "\u001b[32m2026-01-07 21:24:08.811\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtoxicity_detector.config\u001b[0m:\u001b[36mload_env_file\u001b[0m:\u001b[36m183\u001b[0m - \u001b[1mLoaded environment variables from '.env'\u001b[0m\n",
      "\u001b[32m2026-01-07 21:24:08.811\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtoxicity_detector.backend\u001b[0m:\u001b[36mdetect_toxicity\u001b[0m:\u001b[36m43\u001b[0m - \u001b[1mStarting new detection request (uuid: e9400d1f-25f0-49cd-b09c-d104c036b4a4).\u001b[0m\n",
      "\u001b[32m2026-01-07 21:24:08.812\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtoxicity_detector.backend\u001b[0m:\u001b[36mdetect_toxicity\u001b[0m:\u001b[36m47\u001b[0m - \u001b[1mChosen toxicity type: personalized_toxicity\u001b[0m\n",
      "\u001b[32m2026-01-07 21:24:08.812\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtoxicity_detector.backend\u001b[0m:\u001b[36mdetect_toxicity\u001b[0m:\u001b[36m48\u001b[0m - \u001b[1mUsed model: Llama-3.2-3B\u001b[0m\n",
      "\u001b[32m2026-01-07 21:24:08.812\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtoxicity_detector.backend\u001b[0m:\u001b[36mdetect_toxicity\u001b[0m:\u001b[36m49\u001b[0m - \u001b[1mKontextinfo: None\u001b[0m\n",
      "\u001b[32m2026-01-07 21:24:08.813\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtoxicity_detector.backend\u001b[0m:\u001b[36mdetect_toxicity\u001b[0m:\u001b[36m57\u001b[0m - \u001b[1mUsed api key name: tode_debatelab_inference_provider\u001b[0m\n",
      "\u001b[32m2026-01-07 21:24:08.813\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtoxicity_detector.backend\u001b[0m:\u001b[36mdetect_toxicity\u001b[0m:\u001b[36m78\u001b[0m - \u001b[1mModel kwargs: {'max_tokens': 512, 'temperature': 0.2, 'extra_headers': {'X-HF-Bill-To': 'DebateLabKIT'}}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Basierend auf den vorliegenden Analyseergebnissen und der '\n",
      " 'Begriffserläuterung von \"Toxizität\" kann ich folgern, dass der Text \"Peter '\n",
      " 'ist blöd\" zuxischen Inhalt enthält.\\n'\n",
      " '\\n'\n",
      " 'Die Analyse zeigt, dass der Text eine Beleidigung (\"blöd\") enthält, die '\n",
      " 'Peter als dumm oder ungeschickt beschreibt. Diese Beleidigung ist eine Form '\n",
      " 'der persönlichen toxischen Sprache, da sie eine bestimmte Person (Peter) '\n",
      " 'beleidigt oder bedroht. Die spezifische Erwähnung von Peters Namen und die '\n",
      " 'Verwendung des Schimpfworts \"blöd\" als Beleidigung deuten darauf hin, dass '\n",
      " 'der Text eine negative und abwertende Haltung gegenüber Peter einnimmt.\\n'\n",
      " '\\n'\n",
      " 'Darüber hinaus enthält der Text keine Anzeichen dafür, dass er sexuelle '\n",
      " 'Kommentare oder Aufforderungen zu sexuellen Handlungen enthält. Die '\n",
      " 'Indikatorenanalyse ergab, dass der Text keine sexuellen Belästigungen '\n",
      " 'enthält.\\n'\n",
      " '\\n'\n",
      " 'Die Analyse zeigt auch, dass der Text eine abwertende, aber nicht '\n",
      " 'bedrohliche Aussage ist. Die Verwendung des Wortes \"blöd\" als Beleidigung '\n",
      " 'ist eine Form der öffentlichen Abstempelung oder Diskriminierung von Peter, '\n",
      " 'aber sie ist nicht ausreichend, um eine Bedrohung für Peters Sicherheit oder '\n",
      " 'Gesundheit zu begründen.\\n'\n",
      " '\\n'\n",
      " 'Insgesamt kann ich feststellen, dass der Text \"Peter ist blöd\" zuxischen '\n",
      " 'Inhalt enthält, da er eine Beleidigung enthält, die eine bestimmte Person '\n",
      " 'beleidigt oder bedroht. Der Text ist jedoch nicht ausreichend, um eine '\n",
      " 'Bedrohung für Peters Sicherheit oder Gesundheit zu begründen.')\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "from toxicity_detector import detect_toxicity\n",
    "from toxicity_detector.config import PipelineConfig\n",
    "\n",
    "pipeline_config = PipelineConfig(\n",
    "    # We use the Llama-3.2-3B model via the Hugging Face API\n",
    "    # This model is very small and only suitable for demonstration purposes.\n",
    "    # For serious use cases, consider using a larger and newer model.\n",
    "    used_chat_model='Llama-3.2-3B',\n",
    "    local_base_path='.',\n",
    "    result_data_path='result_data',\n",
    "    log_path='logs',\n",
    "    env_file='.env',\n",
    "    models={\n",
    "        'Llama-3.2-3B': {\n",
    "            'name': 'Llama-3.2-3B',\n",
    "            'description': 'Llama-3.2-3B over together.ai',\n",
    "            'base_url': 'https://router.huggingface.co/together/v1',\n",
    "            'model': 'meta-llama/Llama-3.2-3B-Instruct-Turbo',\n",
    "            # Passing the API key directly here for demonstration purposes. \n",
    "            # In practice, you should use environment variables an pass here the \n",
    "            # name of the variable that holds the key, e.g.,\n",
    "            # 'api_key_name': <YOUR_ENV_VARIABLE_NAME>,\n",
    "            #'api_key': <YOUR_HUGGINGFACE_API_KEY>,\n",
    "            'api_key': '',\n",
    "            'llm_chain': 'chat-chain',\n",
    "            'model_kwargs': {\n",
    "                'max_tokens': 512,\n",
    "                'temperature': 0.2,\n",
    "            }\n",
    "        },\n",
    "    }\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the pipeline via `detect_toxicity`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = 'Peter is dumn.'\n",
    "\n",
    "result = detect_toxicity(\n",
    "    input_text=input_text,\n",
    "    user_input_source=None, \n",
    "    toxicity_type='personalized_toxicity',\n",
    "    context_info=None,\n",
    "    pipeline_config=pipeline_config,\n",
    "    serialize_result=True,\n",
    ")\n",
    "pprint(result.answer['analysis_result'])\n",
    "pprint(result.answer['contains_toxicity'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "toxicity-detector",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
